{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Workshop 3 - Paginacja.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabrielMalinowski/GumtreeSpider/blob/master/Workshop_3_Paginacja.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "hKQ3xY5jUUKe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Generatory, Yield"
      ]
    },
    {
      "metadata": {
        "id": "R0FftSW7UUKh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "57b24596-a661-45d7-b522-82a980044015"
      },
      "cell_type": "code",
      "source": [
        "mylist = [0, 1, 4]\n",
        "for i in mylist:\n",
        "    print(i)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "REoOrWiPUUKr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "54518c5f-4173-4abc-844d-77c81adb1408"
      },
      "cell_type": "code",
      "source": [
        "mylist = [x*x for x in range(3)] # lista cała w pamięci\n",
        "for i in mylist:\n",
        "    print(i)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gWenb0YSUUKw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "796be974-4ea5-414b-8cdd-32efb3a1e457"
      },
      "cell_type": "code",
      "source": [
        "mylist = (x*x for x in range(3)) # list comprehension\n",
        "for i in mylist:\n",
        "    print(i)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iujooiifUUK1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0667be4a-a0a2-47bf-c19a-17039295c84d"
      },
      "cell_type": "code",
      "source": [
        "def createGenerator():\n",
        "    mylist = range(3)\n",
        "    for i in mylist:\n",
        "        yield i*i # domknięcie (closure)\n",
        "        \n",
        "for i in createGenerator():\n",
        "    print(i)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gB7kCBbyUUK9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Fail2Ban"
      ]
    },
    {
      "metadata": {
        "id": "RX5SyoIDUUK-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see in my example, I have set up 300 maxretry and 300 for findtime, so, we need to have 300 GETs from the same IP in a time window of 300 seconds to have the originating IP blocked."
      ]
    },
    {
      "metadata": {
        "id": "SuIzBsx5UUK_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Scraper"
      ]
    },
    {
      "metadata": {
        "id": "XcBgiepYUULA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Scrapy doesn’t wait a fixed amount of time between requests, but uses a random interval between 0.5 * DOWNLOAD_DELAY and 1.5 * DOWNLOAD_DELAY."
      ]
    },
    {
      "metadata": {
        "id": "z0_UTYQ_UULC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import scrapy\n",
        "import scrapy.crawler as crawler\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "class MySpider(scrapy.Spider):\n",
        "    name = 'myspider'\n",
        "    start_urls = [\n",
        "        'https://www.gumtree.pl/s-mieszkania-i-domy-sprzedam-i-kupie/mazowieckie/v1c9073l3200001p1'\n",
        "        ]\n",
        "   \n",
        "    custom_settings = {\n",
        "        'DOWNLOAD_DELAY': '4.0',\n",
        "    }\n",
        "\n",
        "    top_url = 'https://www.gumtree.pl'\n",
        "    def parse(self, response):\n",
        "        self.logger.info('Got successful response from {}'.format(response.url))\n",
        "        soup = BeautifulSoup(response.body, 'lxml')\n",
        "        link_tabs = soup.findAll(\"div\", {\"class\": \"result-link\"})\n",
        "        item_urls = []\n",
        "        for tab in link_tabs:\n",
        "            hrefs = tab.findAll(\"a\", {\"class\": \"href-link\"})\n",
        "            for h in hrefs:\n",
        "                item_urls.append(self.top_url + h[\"href\"])\n",
        "            \n",
        "        \n",
        "        for item_url in item_urls:\n",
        "            yield scrapy.Request(item_url, self.parse_item)\n",
        "\n",
        "    def parse_item(self, response):\n",
        "        self.logger.info('Got successful response from {}'.format(response.url))\n",
        "        # <processing code not shown>\n",
        "        #item = MyItem()\n",
        "        # populate `item` fields\n",
        "        # and extract item_details_url\n",
        "        #yield scrapy.Request(item_details_url, self.parse_details, meta={'item': item})\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c-I4mkhwUULF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "process = CrawlerProcess({\n",
        "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
        "})\n",
        "process.crawl(MySpider)\n",
        "process.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NR9pgDlEUULJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import scrapy\n",
        "import scrapy.crawler as crawler\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "class MyPaginatingSpider(scrapy.Spider):\n",
        "    name = 'mypaginatingspider'\n",
        "    start_urls = [\n",
        "        'https://www.gumtree.pl/s-mieszkania-i-domy-sprzedam-i-kupie/mazowieckie/v1c9073l3200001p1'\n",
        "        ]\n",
        "   \n",
        "    custom_settings = {\n",
        "        'DOWNLOAD_DELAY': '6.0',\n",
        "    }\n",
        "\n",
        "    top_url = 'https://www.gumtree.pl'\n",
        "    def parse(self, response):\n",
        "        self.logger.info('Got successful response from {}'.format(response.url))\n",
        "        soup = BeautifulSoup(response.body, 'lxml')\n",
        "        link_tabs = soup.findAll(\"div\", {\"class\": \"result-link\"})\n",
        "        item_urls = []\n",
        "        next_urls = []\n",
        "        for tab in link_tabs:\n",
        "            hrefs = tab.findAll(\"a\", {\"class\": \"href-link\"})\n",
        "            for h in hrefs:\n",
        "                item_urls.append(self.top_url + h[\"href\"])\n",
        "            \n",
        "        nexts = soup.findAll(\"a\", {\"class\": \"next\"})\n",
        "        \n",
        "        for h in nexts:\n",
        "            for h in hrefs:\n",
        "                next_urls.append(self.top_url + h[\"href\"])\n",
        "        \n",
        "        print (next_urls)\n",
        "            \n",
        "        for item_url in item_urls:\n",
        "            yield scrapy.Request(item_url, self.parse_item)\n",
        "\n",
        "        #for next_url in next_urls:\n",
        "        #    yield scrapy.Request(item_url, self.parse)\n",
        "    \n",
        "            \n",
        "    def parse_item(self, response):\n",
        "        self.logger.info('Got successful response from {}'.format(response.url))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IzkuK2TeUULN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "process = CrawlerProcess({\n",
        "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
        "})\n",
        "process.crawl(MyPaginatingSpider)\n",
        "process.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YxYakoluUULQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}